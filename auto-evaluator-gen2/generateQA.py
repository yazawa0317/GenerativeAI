import fitz  # PyMuPDF ã‚’ä½¿ç”¨
import os
import random

from langchain.chains.qa_generation.prompt import CHAT_PROMPT as prompt
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import (
    RunnableLambda,
    RunnableParallel,
    RunnablePassthrough,
)
from langchain_core.runnables.base import RunnableEach
from openai import AzureOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1ï¸âƒ£ PDF ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = "\n".join(page.get_text() for page in doc)
    return text

# 2ï¸âƒ£ LLM ã‚’å‘¼ã³å‡ºã™é–¢æ•°
def call_openai(input_text, model):
    if not isinstance(input_text, str):
        input_text = str(input_text)

    endpoint = os.getenv("AZURE_ENDPOINT_URL")
    subscription_key = os.getenv("AZURE_OPENAI_API_KEY")
    api_version = os.getenv("OPENAI_API_VERSION")


    client = AzureOpenAI(
        azure_endpoint=endpoint,
        api_key=subscription_key,
        api_version=api_version,
    )

    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸå…¥åŠ›ã‹ã‚‰è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’ä½œæˆã—ã¾ã™ã€‚ãªã‚‹ã¹ãå…·ä½“çš„ãªæ•°å­—ãŒå›ç­”ã«å«ã¾ã‚Œã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚å¿…ãšæ—¥æœ¬èªã«ã—ã¦ãã ã•ã„ã€‚"},
            {"role": "user", "content": input_text}
        ]
    )
    return response.choices[0].message.content

# 3ï¸âƒ£ ãƒ¡ã‚¤ãƒ³ã®é–¢æ•°ï¼ˆRunnableã‚’ç¶­æŒï¼‰
def generateQA(text, model='gpt-4o', chunk=512, overlap=128, num_questions=5):
    # â¶ ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        separators=["\n", "ã€‚", "ã€", " "],
        encoding_name='o200k_base',
        chunk_size=chunk,
        chunk_overlap=overlap,
    )

    def split_text_as_string(text):
        docs = text_splitter.create_documents([text])
        return [doc.page_content for doc in docs]

    def split_text_with_sampling(text):
        """RecursiveCharacterTextSplitter + ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒŸãƒƒã‚¯ã‚¹"""
        docs = text_splitter.create_documents([text])
        split_texts = [doc.page_content for doc in docs]

        # ğŸ¯ ã“ã“ã§ãƒ©ãƒ³ãƒ€ãƒ ã« num_questions å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é¸æŠ
        if num_questions < len(split_texts):
            selected_texts = random.sample(split_texts, num_questions)
        else:
            selected_texts = split_texts  # ååˆ†ãªæ•°ãŒãªã„å ´åˆã¯å…¨ã¦

        return selected_texts


#    split_text = RunnableLambda(split_text_as_string)
    split_text = RunnableLambda(split_text_with_sampling)

    # â· LLM å‘¼ã³å‡ºã—ã‚’ Runnable åŒ–
    llm_runnable = RunnableLambda(lambda input_text: call_openai(input_text, model))

    # â¸ Runnable ãƒã‚§ãƒ¼ãƒ³ã‚’å®šç¾©
    chain = RunnableParallel(
        text=RunnablePassthrough(),
        questions=split_text | RunnableEach(bound=prompt | llm_runnable | JsonOutputParser())
    )

    # â¹ å®Ÿè¡Œã—ã¦çµæœã‚’å–å¾—
    result = chain.invoke(text)
    
    return result.get('questions', [])

# 4ï¸âƒ£ å®Ÿè¡Œä¾‹
if __name__ == "__main__":

    from dotenv import load_dotenv
    # .envã‹ã‚‰AOAIæ¥ç¶šã‚ˆã†ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ç’°å¢ƒå¤‰æ•°ã«ã‚»ãƒƒãƒˆ
    load_dotenv()

    pdf_text = extract_text_from_pdf("C:\\Develop\\python\\GenAI\\RAG\\Spliter\\doc\\test.pdf")
    results = generateQA(text=pdf_text, model="gpt-4o", chunk=1024, overlap=128, num_questions=10)

    for qa in results:
        print(qa)
